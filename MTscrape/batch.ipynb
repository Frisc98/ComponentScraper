{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing, requests\n",
    "from bs4 import BeautifulSoup\n",
    "from queue import Queue, Empty\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "URL_base = \"https://www.tevetron.hr/hr/webshop/ic/16/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTWebCrawler:\n",
    "\n",
    "    def __init__(self, URL_base) -> None:\n",
    "        self.URL_base = URL_base\n",
    "        self.rootURL = '{}://{}/{}'.format(urlparse(self.URL_base).scheme,\n",
    "                                        urlparse(self.URL_base).netloc,\n",
    "                                        urlparse(self.URL_base).path)\n",
    "        self.pool = ThreadPoolExecutor(max_workers=4)\n",
    "        self.scrapedPages = set([])\n",
    "        self.crawlQueue = Queue()\n",
    "        self.crawlQueue.put(self.URL_base)\n",
    "        print(f'Root URL initialized to {self.rootURL}')\n",
    "\n",
    "    def scrapePage(self, url):\n",
    "        try:\n",
    "            res = requests.get(url, timeout=(3, 30))\n",
    "            return res\n",
    "        except requests.RequestException:\n",
    "            return\n",
    "\n",
    "    def runWebCrawler(self):\n",
    "        while True:\n",
    "            try:\n",
    "                print(\"\\n Name of current process: \", multiprocessing.current_process().name, \"\\n\")\n",
    "                targetURL = self.crawlQueue.get(timeout=60)\n",
    "\n",
    "                if targetURL not in self.scrapedPages:\n",
    "                    print(\"Scraping URL: {}\".format(targetURL))\n",
    "                    self.scrapedPages.add(targetURL)\n",
    "                    job = self.pool.submit(self.scrapePage, targetURL)\n",
    "                    job.add_done_callback(self.postScrapeCallback)\n",
    "            except Empty:\n",
    "                return\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "    \n",
    "    def scrapeInfo(self, html):\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        products = soup.find_all(\"div\", class_=\"product-inner\")\n",
    "        \n",
    "        for item in products:\n",
    "            productNumber = item.find(\"h3\").text.strip()\n",
    "            productPrice = item.find(\"span\", class_ = \"cprice1\").text.strip()\n",
    "            productPackage = item.find(\"h4\").text.strip()\n",
    "            productAvailability = item.find(\"span\", class_=\"raspolozivost_2 da\").text.strip() if item.find(\"span\", class_=\"raspolozivost_2 da\") != None else item.find(\"span\", class_=\"raspolozivost_2 ne\").text.strip()\n",
    "            \n",
    "        print(f'PN: {productNumber} Price: {productPrice}')\n",
    "    \n",
    "    def parseLinks(self, html):\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        Anchor_Tags = soup.find_all('a', class_= 'page_numbers', href=True)\n",
    "        print(Anchor_Tags)\n",
    "        for link in Anchor_Tags:\n",
    "            url = link['href']\n",
    "\n",
    "            if self.URL_base in url:\n",
    "\n",
    "                url = urljoin(self.rootURL, url)\n",
    "                \n",
    "                if url not in self.scrapedPages:\n",
    "                    self.crawlQueue.put(url)\n",
    "\n",
    "            # if url.startswith('/') or url.startswith(self.rootURL):\n",
    "            #     url = urljoin(self.rootURL, url)\n",
    "                \n",
    "            #     if url not in self.scrapedPages:\n",
    "            #         self.crawlQueue.put(url)\n",
    "\n",
    "    def postScrapeCallback(self, res):\n",
    "        result = res.result()\n",
    "        \n",
    "        if result and result.status_code == 200:\n",
    "            self.parseLinks(result.text)\n",
    "            self.scrapeInfo(result.text)\n",
    "\n",
    "    def info(self):\n",
    "        print('\\n Seed URL is: ', self.URL_base, '\\n')\n",
    "        print('Scraped pages are: ', self.scrapedPages, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root URL initialized to https://www.tevetron.hr//hr/webshop/ic/16/\n",
      "\n",
      " Name of current process:  MainProcess \n",
      "\n",
      "Scraping URL: https://www.tevetron.hr/hr/webshop/ic/16/\n",
      "\n",
      " Name of current process:  MainProcess \n",
      "\n",
      "[]\n",
      "PN: XR2206D-SMD-EXAR Price: 11,29 â¬ (85,06 kn)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/filip/Documents/ComponentScraper/MTscrape/batch.ipynb Cell 3\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/filip/Documents/ComponentScraper/MTscrape/batch.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/filip/Documents/ComponentScraper/MTscrape/batch.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     cc \u001b[39m=\u001b[39m MTWebCrawler(URL_base)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/filip/Documents/ComponentScraper/MTscrape/batch.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     cc\u001b[39m.\u001b[39;49mrunWebCrawler()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/filip/Documents/ComponentScraper/MTscrape/batch.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     cc\u001b[39m.\u001b[39minfo()\n",
      "\u001b[1;32m/home/filip/Documents/ComponentScraper/MTscrape/batch.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/filip/Documents/ComponentScraper/MTscrape/batch.ipynb#W2sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/filip/Documents/ComponentScraper/MTscrape/batch.ipynb#W2sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m Name of current process: \u001b[39m\u001b[39m\"\u001b[39m, multiprocessing\u001b[39m.\u001b[39mcurrent_process()\u001b[39m.\u001b[39mname, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/filip/Documents/ComponentScraper/MTscrape/batch.ipynb#W2sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     targetURL \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcrawlQueue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39m60\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/filip/Documents/ComponentScraper/MTscrape/batch.ipynb#W2sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     \u001b[39mif\u001b[39;00m targetURL \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscrapedPages:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/filip/Documents/ComponentScraper/MTscrape/batch.ipynb#W2sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mScraping URL: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(targetURL))\n",
      "File \u001b[0;32m/usr/lib/python3.10/queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[39mif\u001b[39;00m remaining \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m:\n\u001b[1;32m    179\u001b[0m             \u001b[39mraise\u001b[39;00m Empty\n\u001b[0;32m--> 180\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnot_empty\u001b[39m.\u001b[39;49mwait(remaining)\n\u001b[1;32m    181\u001b[0m item \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get()\n\u001b[1;32m    182\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnot_full\u001b[39m.\u001b[39mnotify()\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39;49macquire(\u001b[39mTrue\u001b[39;49;00m, timeout)\n\u001b[1;32m    325\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39macquire(\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    cc = MTWebCrawler(URL_base)\n",
    "    cc.runWebCrawler()\n",
    "    cc.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
